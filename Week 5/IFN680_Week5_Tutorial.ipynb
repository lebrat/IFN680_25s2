{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# (Continued) ML that can See: Supervised Learning with Images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import torch which has many of the functions to build deep learning models and to train them\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#import torchvision, which was lots of functions for loading and working with image data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#this is a nice progress bar representation that will be good to measure progress during training\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d496-b334-499f-b57f-da86a441f925",
   "metadata": {},
   "source": [
    "# Ingredient 1: The Data\n",
    "\n",
    "We're going to use the same dataset as last week, with images from 20 different dog breeds we want to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ab9a4-8c7a-474a-916a-e874877a452d",
   "metadata": {},
   "source": [
    "As we have learnt to do in the past couple of weeks, we will:\n",
    "1. Initialise a transformation for the dataset that:\n",
    "    1. [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html) -- this converts a PIL image or numpy array to a tensor while scaling the pixel values to the range [0, 1].\n",
    "    2. [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) -- this resizes an input image to the specified size (height, width).\n",
    "    Resize is important as it ensures the dimensions remain compatible throughout the network, allowing proper operations at each layer and maintaining the required dimensions for the final fully connected layers in the network.\n",
    "    3. [transforms.Normalize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html) -- this standardizes the pixel values of a tensor image by subtracting the mean and dividing by the standard deviation along the input channels.\n",
    "2. We will then load the datasets with [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) -- this loads image datasets from folders, assigning labels automatically based on subdirectories.\n",
    "3. Create a [torch.utils.data.DataLoader()](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "    1. First argument is the dataset.\n",
    "    2. Optional argument *batch_size* is the batch size to test the model with. How many data points will the model be tested on in parallel?\n",
    "    3. Optional argument *shuffle* controls whether data is randomly shuffled before taking from the dataset.\n",
    "    4. Optional argument *num_workers* is how many subprocesses are used to load data from the dataset -- it can make loading the data faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7e8dd-08a7-43f7-94bd-bbc3af8bb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_means = (0.485, 0.456, 0.406)\n",
    "imagenet_stds = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)), \n",
    "     transforms.Normalize(imagenet_means, imagenet_stds)])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('./stanford_dogs_subset/train', transform = transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder('./stanford_dogs_subset/val', transform = transform)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers = 1)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a65b86-b0f3-4484-a1c9-11106e4c41f9",
   "metadata": {},
   "source": [
    "Last week we explored that adding random data transformations during training can help prevent overfitting.\n",
    "\n",
    "Below is a list of some common augmentations, the cell below is simply using a RandomHorizontalFlip augmentation.\n",
    "\n",
    "1. [transforms.RandomResizedCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomResizedCrop.html) -- this function randomly grabs a portion of the image (crops) and then resizes to the desired image size. By default, the crop can be anywhere between 8% to 100% of the image original area -- this is a little strict, I'm going to choose between 50% and 100% of the image area.\n",
    "2. [transforms.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html) -- randomly flips an image horizontally. Useful for tasks where horizontal orientation doesn't change the meaning.\n",
    "3. [transforms.RandomVerticalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomVerticalFlip.html) -- randomly flips an image vertically. Useful for tasks where vertical orientation doesn't change the meaning.\n",
    "4. [transforms.RandomRotation](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomRotation.html) -- randomly rotates an image by a specified angle. Can simulate variations in viewpoint.\n",
    "5. [transforms.ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) -- randomly changes brightness, contrast, saturation, and hue of an image. Helps the model to be robust to different lighting conditions.\n",
    "\n",
    "\n",
    "Each of the 'Random' transformations will be sequentially applied to an input image, with different transformations of different severities - the severity of the transformation is the random component. When you chain together multiple different types of 'Random' transformations, we can end up with a huge variation of different images from our training dataset.\n",
    "\n",
    "**In your own time, experiment with adding different augmentations and observe how performance changes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5cfdb-92c9-4941-acfd-bca354741eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add more transforms in here\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "\n",
    "class_labels = train_dataset.classes\n",
    "#visualise the train dataset with these transforms\n",
    "data = next(iter(trainloader))\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "for idx in range(5):\n",
    "    im = data[0][idx]\n",
    "    lbl = data[1][idx]\n",
    "    im = train_transform(im)\n",
    "    train_image = (im.numpy())/2 + 0.5\n",
    "    label = class_labels[lbl]\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    ax[idx].imshow(train_image)\n",
    "    ax[idx].set_axis_off()\n",
    "    ax[idx].set_title(label.split('-')[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b7ba5-55b5-4350-abe2-9eb88cc38537",
   "metadata": {},
   "source": [
    "# Ingredient 2: The Model\n",
    "\n",
    "This week we will again use a pretrained ResNet18, that has been trained on ImageNet, but we will be freezing certain parameters in the model so that their weights do not update. We will do this to try and prevent the model from overfitting to the new, small dataset. \n",
    "\n",
    "You can see all the models built into torchvision [here](https://pytorch.org/vision/stable/models.html#classification).\n",
    "\n",
    "When we create a model for transfer learning, we should follow these steps:\n",
    "1. Initialise the model with pretrained weights.\n",
    "2. Adapt the architecture for the new number of classes in our new dataset by changing the final linear layer.\n",
    "3. If necessary, freeze any weights.\n",
    "4. Move the model to the GPU if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2943b5-399d-4e6b-970b-909859cd440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(num_classes, freeze_backbone = False):\n",
    "    #### Step 1: Initialise the model with pretrained weights.\n",
    "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    #### Step 2: Adapt the architecture for the new number of classes.\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    #### Step 3: If necessary, freeze any weights.\n",
    "    if freeze_backbone: \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the parameters of the last fully connected layer\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    #### Step 4: Move the model to the GPU if available\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "resnet_frozen = setup_model(20, True)\n",
    "print(resnet_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61498737-b776-4a8d-80d6-e8d2f72f90f9",
   "metadata": {},
   "source": [
    "# Training Time: Transfer Learning with a frozen backbone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d207335-fe2e-4b0b-ac51-6e41528b136b",
   "metadata": {},
   "source": [
    "Now that we've initialised our model, adapted it's architecture for our new training dataset, and initialised the other elements of training (stochastic gradient descent optimizer and cross-entropy loss), we can start to train our model.\n",
    "\n",
    "We're going to use a fine-tuning approach, where the model's parameters are adjusted slightly to adapt its learned features to the specific nuances of the new task or domain. We're going to adjust the parameters in only the final linear layer of the network (i.e. all other layers are frozen).\n",
    "\n",
    "In the lecture, we reviewed that we can train our model by doing the following:\n",
    "\n",
    "1. Initialise the model.\n",
    "2. Define a loss function (or cost function or objective).\n",
    "3. Initialise the SGD optimizer.\n",
    "4. For n epochs (e.g. loss converged/stops changing):\n",
    "    1. Put the model in \"train\" mode with model.train() \n",
    "    2. Training loop: For all batches in the training dataset:\n",
    "        1. Apply any training data transformations to input.\n",
    "        2. Perform a forward pass to find a prediction.\n",
    "        3. Calculate the loss + accuracy.\n",
    "        4. Perform a backward pass to calculate loss gradients with respect to the parameters.\n",
    "        5. Update the parameters with SGD.\n",
    "    4. Put the model in \"eval\" mode with model.eval()\n",
    "    5. Validation loop: For all batches in the validation dataset:\n",
    "        1. Perform a forward pass to find a prediction.\n",
    "        2. Calculate the loss + accuracy.\n",
    "\n",
    "Below, we've done steps 1-3 and 4A-B. \n",
    "\n",
    "**Your turn:** Implement 4C-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e310fc6-cb70-4354-8745-7b5393ef1d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "\n",
    "#any hyperparameters\n",
    "lr = 0.001\n",
    "total_epochs = 10\n",
    "\n",
    "#Step 1: Initialise the model.\n",
    "resnet_frozen = setup_model(20, True)\n",
    "# Step 2: Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Step 3: Initialise the SGD optimizer.\n",
    "optimizer = optim.SGD(resnet_frozen.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "#Step 4: For n epochs (e.g. loss converged/stops changing)\n",
    "total_train_loss = []\n",
    "total_train_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(total_epochs):    \n",
    "    #Step 4A: Put the model in \"train\" mode\n",
    "    resnet_frozen.train() \n",
    "\n",
    "    #Step 4B: Training loop: For all batches in the training dataset\n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in  tqdm.tqdm(enumerate(trainloader, 0), total = len(trainloader), desc = f'Epoch {epoch+1} - training phase'):\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = train_transform(inputs)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet_frozen(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += [loss.cpu().item()]\n",
    "        \n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        correct += torch.sum(predicted == labels).cpu().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    train_accuracy = correct/total\n",
    "\n",
    "    total_train_loss += [mean_train_loss]\n",
    "    total_train_acc += [train_accuracy]\n",
    "    \n",
    "    #Step 4C: Put the model in \"eval\" mode\n",
    "    \n",
    "\n",
    "    #Step 4D: Validation loop: For all batches in the validation dataset\n",
    "\n",
    "\n",
    "plt.plot(total_train_loss, label = 'Train')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(total_train_acc, label = 'Train')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40216fd-2172-4b02-8916-e332b1eed54a",
   "metadata": {},
   "source": [
    "## Accounting for class imbalance\n",
    "\n",
    "When we're training our model, every image in the batch is treated equally important for the learning process.\n",
    "\n",
    "So what happens when we have some classes with very little data, and others with **loads** of data? Potentially our model will overly focus on performing well on the class with lots of data, and neglect the class with less data.\n",
    "\n",
    "We can account for this using Pytorch's [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler), which can be used in the DataLoader class.\n",
    "\n",
    "To do this, we need to choose weights for the sampler (these dictate how often certain samples are used in a batch) based on our class imbalance, create the WeightedRandomSampler, and reload our Dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea64bf-7b7b-403f-9335-bc0e804272b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset.targets\n",
    "\n",
    "lbls, counts = np.unique(train_labels, return_counts = True)\n",
    "\n",
    "weighting = torch.DoubleTensor([1/x for x in counts])\n",
    "sample_weights = weighting[train_dataset.targets]\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(train_dataset))\n",
    "balanced_trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          sampler = sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c58d0d-b6dc-452f-bdb4-005a5443b317",
   "metadata": {},
   "source": [
    "The code below visualises the distribution of samples for each class in the training dataset, when used as is, or when using the WeightedRandomSampler to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d7dce-e0bc-4a98-be97-5b002459b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_dataset.targets\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "balanced_classes = []\n",
    "for data in balanced_trainloader:\n",
    "    ims, tgts = data\n",
    "    balanced_classes += tgts.tolist()\n",
    "\n",
    "\n",
    "plt.hist([train_classes, balanced_classes], bins = 20, label = ['Train', 'Balanced'])\n",
    "plt.xticks([i for i in range(20)], class_names, rotation = 90)\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bfeb6-4750-44af-a056-c4228bc7d380",
   "metadata": {},
   "source": [
    "**Try re-training the model with the new balanced_trainloader and see how the performance changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c05f63-90a8-44b3-bbae-4ec824d1f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e39aa4f3-f145-45ee-a43d-b8ac360f0430",
   "metadata": {},
   "source": [
    "# Leveraging Foundation Models for Image Classification\n",
    "\n",
    "## A new model!\n",
    "\n",
    "We are going to use the foundation model DINOv2 as a feature extractor -- this means we will provide the model with images, collect the features, and then use some other form of machine learning or deep learning to classify these features into different classes!\n",
    "\n",
    "Firstly, we can load the model below from the [torch hub](https://pytorch.org/hub/).\n",
    "\n",
    "You can see the list of DINOv2 architectures at [this link](https://github.com/facebookresearch/dinov2/blob/main/MODEL_CARD.md). We're going to use one of the smaller DINOv2 architectures that still gives great performance but is not too computationally expensive to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ea6e9-af86-420f-a199-c40eb7e38e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dino.eval()\n",
    "dino.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6d66d-6532-4788-b57f-b2de42ef1a58",
   "metadata": {},
   "source": [
    "DINOv2 is like our other CNN models -- to process data, it should be a Tensor, should be sized 224x224, and should be normalized.\n",
    "\n",
    "This means that we can continue using our existing dataloader from earlier in the tutorial!\n",
    "\n",
    "Let's pass some data through DINOv2 and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd57aa8-80d9-415d-a62e-10f237695eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainloader:\n",
    "    inputs, labels = data\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    feature = dino(inputs)\n",
    "\n",
    "    print(feature.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae774bac-d670-4bbb-81bf-dc4d807aaee0",
   "metadata": {},
   "source": [
    "## Linear Classification from DINOv2 Features\n",
    "\n",
    "Adapt the linear classifier below so that it can take DINOv2 features as input, and return a set of class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031d520-5379-45b5-8cb2-34031be99664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(..., ...)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "       \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaf05a-8bb1-4f47-9b43-7a7b5b660d2d",
   "metadata": {},
   "source": [
    "Now, create a training loop where we learn the parameters for the linear classifier from the DINOv2 features. \n",
    "\n",
    "**You can adapt from the above training/val code, BUT make sure that you make the following changes:**\n",
    "* create a LinearClassifier\n",
    "* link the SGD with the LinearClassifier\n",
    "* everywhere you previously tested with ResNet, change to (1) use DINOv2 to reduce the image to a feature, and then (2) pass the feature through the linear classifier\n",
    "* save the weights of the LinearClassifier\n",
    "\n",
    "We will also keep the linear classifier on the cpu, as we are training only a very small linear classifier.\n",
    "\n",
    "**CUDA Run out of memory?**\n",
    "* If you get a CUDA out of memory error, you will need to restart your kernel and run the notebook again. Make sure to save your progress.\n",
    "* You can stop this from happening by reducing the batch size in your data loader -- i.e. how many images being moved onto the GPU -- or by using smaller models.\n",
    "* If this does not work, you may need to train on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223502f5-5fb1-49f0-8297-9759c4b40ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "balanced_trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          sampler = sampler, num_workers = 1)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0723341-dfa1-45a4-a05c-cfd25d581407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c24174-c312-46bb-85de-995073eb3d22",
   "metadata": {},
   "source": [
    "**Food for thought:** \n",
    "* Could you build a KNN classifier using DINOv2 features as the input? Why might this be preferable to a deep learning model?\n",
    "* Could you build a neural network using DINOv2 features as the input? Why might this be preferable to a linear classifier?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
