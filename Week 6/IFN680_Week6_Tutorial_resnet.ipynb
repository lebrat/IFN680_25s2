{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Exploring Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "#import torch which has many of the functions to build deep learning models and to train them\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "#import torchvision, which was lots of functions for loading and working with image data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#this is a nice progress bar representation that will be good to measure progress during training\n",
    "import tqdm\n",
    "\n",
    "#for creating confusion matrices from predictions\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b180fc-6f37-4da2-be3a-2e9a62e08f69",
   "metadata": {},
   "source": [
    "# 1) Splitting into a training and validation dataset\n",
    "\n",
    "As we did in Week 5, we're going to use the dataset with 20 dog species. \n",
    "However, I'm using the 'train' folder as train and validation dataset, while the 'val' folder will be our test dataset where we will compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4fde18-aa02-4d94-9531-54f87f2511eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_means = (0.485, 0.456, 0.406)\n",
    "imagenet_stds = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)), \n",
    "     transforms.Normalize(imagenet_means, imagenet_stds)])\n",
    "\n",
    "trainval_dataset = torchvision.datasets.ImageFolder('../Week 5/stanford_dogs_subset/train', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac994cc",
   "metadata": {},
   "source": [
    "Let's look the number of samples per class in the ''stanford_dogs_subset/train''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b712654",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(trainval_dataset.classes)\n",
    "plt.figure(figsize=(10,5))\n",
    "g = sns.catplot(x=trainval_dataset.targets, kind=\"count\", height=4, aspect=1)\n",
    "g.set_axis_labels(\"Class Label\", \"Count\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_xticklabels(trainval_dataset.classes, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d50cd5",
   "metadata": {},
   "source": [
    "As we know, the classes are not uniformly distributed; different classes have different numbers of samples. How should we split this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626ae78-2ad0-4126-a0f5-798d8f825e8c",
   "metadata": {},
   "source": [
    "## 1a) Using stratify to split into train and val\n",
    "\n",
    "We can use [sklearn.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with the stratify option to split a dataset into train and validation while keeping the class distribution unchanged.\n",
    "\n",
    "**TASK:** Using the function above, complete the code to create train and val datasets, with an 80/20 split of the trainval_dataset, ensuring the class distribution is preserved across Train and Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1afeb-780c-4b95-8508-7d3dbbc4dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = 0.8\n",
    "val_portion = 0.2\n",
    "\n",
    "# Step 1: create indices for each sample in the dataset and collect their labels\n",
    "\n",
    "# Step 2: split indices with train_test_split\n",
    "\n",
    "# Step 3: wrap them as Subsets\n",
    "\n",
    "\n",
    "print(f\"Number of samples: train_dataset={len(train_dataset)} and validation={len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec163f2",
   "metadata": {},
   "source": [
    "Let's see how the class distribution looks like in our splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb0101-c2ba-4d5f-8db4-ce315c0ab0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(trainval_dataset.classes)\n",
    "labels = [data[1] for data in train_dataset] + [data[1] for data in val_dataset]\n",
    "splits = ([\"Train\"] * len(train_dataset)) + ([\"Val\"] * len(val_dataset))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "g = sns.catplot(x=labels, kind=\"count\", col=splits, height=4, aspect=1, sharey=True)\n",
    "g.set_axis_labels(\"Class Label\", \"Count\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_xticklabels(trainval_dataset.classes, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c476f",
   "metadata": {},
   "source": [
    "# 2) Model \n",
    "We are going to use the 'ResNet' from last week. As we have learned, \n",
    "1. Setup model\n",
    "2. Setup dataloaders\n",
    "3. Execute Training loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5320279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1 initialize pre-trained backbone\n",
    "resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#### Step 2: Adapt the architecture for the new number of classes.\n",
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 20)\n",
    "\n",
    "#### Step 3: If necessary, freeze any weights.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the parameters of the last fully connected layer\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d7e0f",
   "metadata": {},
   "source": [
    "We are going to use the balanced dataloader for training and a normal dataloader for validation on top of our data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d61d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_labels = np.array([data[1] for data in train_dataset])\n",
    "lbls, counts = np.unique(train_labels, return_counts = True)\n",
    "weighting = torch.DoubleTensor([1/x for x in counts])\n",
    "sample_weights = weighting[train_labels]\n",
    "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(train_dataset))\n",
    "\n",
    "balanced_trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler = sampler)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1da40",
   "metadata": {},
   "source": [
    "Just Run the training to get the best model possibel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#any hyperparameters\n",
    "lr = 0.001\n",
    "total_epochs = 10 # five epochs is enough to show improvements\n",
    "\n",
    "#Step 1: Initialise the model.\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Step 2: Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Step 3: Initialise the SGD optimizer.\n",
    "optimizer = optim.SGD(resnet.fc.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "#Step 4: For n epochs (e.g. loss converged/stops changing)\n",
    "total_train_loss, total_val_loss = [], []\n",
    "total_train_acc, total_val_acc = [], []\n",
    "best_acc = 0\n",
    "for epoch in range(total_epochs):    \n",
    "    #Step 4A: Put the model in \"train\" mode\n",
    "    resnet.train() \n",
    "\n",
    "    #Step 4B: Training loop: For all batches in the training dataset\n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in  tqdm.tqdm(enumerate(balanced_trainloader, 0), total = len(balanced_trainloader), desc = f'Epoch {epoch+1} - training phase'):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # inputs = train_transform(inputs)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += [loss.cpu().item()]\n",
    "        \n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        correct += torch.sum(predicted == labels).cpu().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    train_accuracy = correct/total\n",
    "    print(f\"Training {epoch+1}: loss={mean_train_loss:.3f} acc={train_accuracy:.3f}\")\n",
    "\n",
    "    total_train_loss += [mean_train_loss]\n",
    "    total_train_acc += [train_accuracy]\n",
    "    \n",
    "    #Step 4C: Put the model in \"eval\" mode\n",
    "    resnet.eval()    \n",
    "\n",
    "    #Step 4D: Validation loop: For all batches in the validation dataset\n",
    "    with torch.no_grad(): # not build the computation graph for backpropagation, and thus, no gradients will be computed or stored for the tensors involved in those operations.\n",
    "        val_loss, val_correct, val_total = 0.0, 0.0, 0.0\n",
    "        for i, data in  tqdm.tqdm(enumerate(valloader, 0), total = len(valloader), desc = f'Epoch {epoch+1} - validation phase'):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = resnet(inputs)\n",
    "\n",
    "            predicted = torch.argmax(outputs, axis = 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.cpu().item() * inputs.size(0)\n",
    "    \n",
    "            val_correct += torch.sum(predicted == labels).cpu().item()\n",
    "            val_total += inputs.size(0)\n",
    "    \n",
    "        mean_val_loss = val_loss / val_total\n",
    "        val_accuracy = val_correct / val_total\n",
    "        print(f\"Validation {epoch+1}: loss={mean_val_loss:.3f} acc={val_accuracy:.3f}\")\n",
    "    \n",
    "        if val_accuracy > best_acc:\n",
    "            torch.save(resnet.state_dict(), \"resnet_best.pth\")\n",
    "            best_acc = val_accuracy\n",
    "            print(f\"best model found with {best_acc:.3f}\")\n",
    "\n",
    "        total_val_loss.append(mean_val_loss)\n",
    "        total_val_acc.append(val_accuracy)\n",
    "\n",
    "\n",
    "plt.plot(total_train_loss, label = 'Train')\n",
    "plt.plot(total_val_loss, label = 'val')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(total_train_acc, label = 'Train')\n",
    "plt.plot(total_val_acc, label = 'Train')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a1ee9",
   "metadata": {},
   "source": [
    "Should we still be using Accuracy as validation metric? \n",
    "WARNING: A classifier can achieve high accuracy by only predicting the majority class when imbalance exists. \n",
    "Let's have a look in other metrics. First using a simpler binary classification problem, later we come back to the multiclass case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6870d",
   "metadata": {},
   "source": [
    "# 3) Hounds Vs Others (Binary Classification)\n",
    "\n",
    "We will try to classify hound species versus other species. **A hound species is any dog species with the word 'hound' in its name.** \n",
    "Below, we will evaluate the performance of our model in this task.\n",
    "Let's start by running the model in our test set and collecting the ground-truths, predictions, and probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1224acf",
   "metadata": {},
   "source": [
    " Set up the test data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('../Week 5/stanford_dogs_subset/val', transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(test_dataset.classes)\n",
    "plt.figure(figsize=(10,5))\n",
    "g = sns.catplot(x=test_dataset.targets, kind=\"count\", height=4, aspect=1)\n",
    "g.set_axis_labels(\"Class Label\", \"Count\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_xticklabels(test_dataset.classes, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d641d3",
   "metadata": {},
   "source": [
    "Load the weights of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.load_state_dict(torch.load('resnet_best.pth', map_location = 'cpu'))\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca1e3a",
   "metadata": {},
   "source": [
    "runing our classifier through the test set to collect ground-truth labels, predicted classes, and estimated porbabilities.\n",
    "\n",
    "**TASK:** Write a loop to run the model on all samples of the test set. Also, collect the labels and predicted probability vectors for all of them in numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a402fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts, probs = [], []\n",
    "with torch.no_grad():\n",
    "    resnet.eval()\n",
    "    for i, data in tqdm.tqdm(enumerate(testloader), total=len(testloader), desc=\"Evaluating\"):\n",
    "        # read data\n",
    "                \n",
    "        # run model to get probability outputs        \n",
    "                \n",
    "        # collect into list\n",
    "        \n",
    "gts, probs = np.concatenate(gts).astype(np.int32), np.vstack(probs).astype(np.float32)\n",
    "print(f\"{len(gts)}/{probs.shape} labels and predictions collected\")\n",
    "print(gts[0], probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c33b18",
   "metadata": {},
   "source": [
    "Finnally, let's transform the multi-class predictions into binary prediction (Hounds vs Other).\n",
    "\n",
    "**TASK:** Combine the 20 class multi-class predictions into a binary prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "hound_labels = [idx for idx, name in enumerate(test_dataset.classes) if  'hound' in name]\n",
    "\n",
    "# set True for samples that are hounds\n",
    "gts_hounds = ...\n",
    "# sum the probability of a sample being a hound\n",
    "probs_hounds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826d76",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We can use [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) from sklearn.metrics to create a confusion matrix.\n",
    "\n",
    "Below, we use the ConfusionMatrixDisplay.from_predictions() function to create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_hounds  = probs_hounds > 0.5\n",
    "ConfusionMatrixDisplay.from_predictions(gts_hounds, preds_hounds, display_labels=['Other', 'Hound'], xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa5bbc",
   "metadata": {},
   "source": [
    "Can we do it by ourselves? \n",
    "\n",
    "In a **binary classification** problem, the **confusion matrix** is defined by the following counts:\n",
    "\n",
    "|                  | Predicted Negative | Predicted Positive |\n",
    "|------------------|-------------------|-------------------|\n",
    "| **Actual Negative** | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "\n",
    "- TP (True Positive): Model predicts positive, and the true label is positive.\n",
    "- FP (False Positive): Model predicts positive, but the true label is negative.\n",
    "- TN (True Negative): Model predicts negative, and the true label is negative.\n",
    "- FN (False Negative): Model predicts negative, but the true label is positive.\n",
    "\n",
    "OBS: Sometimes this matrix can look transposed.\n",
    "\n",
    "**TASK:** Compute these counts using only numpy and plot a confusion matrix using matplotlib: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs make sure gts_hounds and preds_hounds are boolean vectors \n",
    "preds_hounds  = probs_hounds > 0.5\n",
    "\n",
    "# vectorized code for counting\n",
    "tp = ...\n",
    "fp = ...\n",
    "tn = ...\n",
    "fn =  ...\n",
    "\n",
    "cm_counts = np.array([[tn, fp], [fn, tp]]).astype(np.int32)\n",
    "\n",
    "labels = ['Others', 'Hounds']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "im = ax.imshow(cm_counts, cmap='viridis')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(cm_counts.shape[0]):\n",
    "    for j in range(cm_counts.shape[1]):\n",
    "        ax.text(j, i, str(cm_counts[i, j]),\n",
    "                ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Optional: colorbar\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81b39c",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can compute classification metrics and curves that will help us to evaluate our model:\n",
    "\n",
    "###  Precision\n",
    "Precision measures the proportion of correctly predicted positive samples out of all samples predicted as positive.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{TP}{PP}\n",
    "$$\n",
    "\n",
    "- High precision → very few false positives.  \n",
    "- Example: Out of all dogs predicted to be hound dogs, how many really are?\n",
    "\n",
    "\n",
    "### Recall (Sensitivity, True Positive Rate)\n",
    "Recall measures the proportion of correctly predicted positive samples out of all actual positive samples.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$\n",
    "\n",
    "- High recall → very few false negatives.  \n",
    "- Example: Out of all dogs that are actually hound dogs, how many did we detect?\n",
    "\n",
    "\n",
    "###  F-Score (F1-Score)\n",
    "The F-score is the harmonic mean of precision and recall. It balances the two metrics:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- High F1 means both precision and recall are reasonably high.  \n",
    "- Useful when there is **class imbalance**.\n",
    "\n",
    "Let's compute these metrics for hounds vs others problem.\n",
    "\n",
    "**TASK:** From the confusion matrix counts computed above, compute precision, recall and f-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the equations above\n",
    "prec = ...\n",
    "rec = ...\n",
    "fscore = ...\n",
    "print(f\"precision={prec:.2f}, recall={rec:.2f}, f-score={fscore:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92a95",
   "metadata": {},
   "source": [
    "However, this performance is calculated for a threshold of 0.5. You can re-run the code using different thresholds. By setting different thresholds, we obtain different results. Which threshold should we choose, and how can we compare or evaluate performance across all possible thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690fcb9",
   "metadata": {},
   "source": [
    "To answer this question, let's have a look in the distribution of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "num_bins = 10  # number of bins (e.g., 0.0–0.1, 0.1–0.2, ..., 0.9–1.0)\n",
    "hounds_counts, hounds_bin_edges = np.histogram(probs_hounds[gts_hounds], bins=num_bins, range=(0, 1))\n",
    "others_counts, others_bin_edges = np.histogram(probs_hounds[~gts_hounds], bins=num_bins, range=(0, 1))\n",
    "\n",
    "# Plot histogram using matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(hounds_bin_edges[:-1], hounds_counts, width=hounds_bin_edges[1] - hounds_bin_edges[0], edgecolor='black', align='edge', label='Hounds', alpha=0.5)\n",
    "plt.bar(others_bin_edges[:-1], others_counts, width=others_bin_edges[1] - others_bin_edges[0], edgecolor='black', align='edge', label='Others', alpha=0.5)\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Frequency (number of samples)')\n",
    "plt.title('Distribution of predicted probabilities')\n",
    "plt.xticks(np.linspace(0, 1, num_bins + 1))  # ticks at each bin edge\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5b55b",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "The **ROC (Receiver Operating Characteristic) curve** is a tool to evaluate the performance of a **binary classifier** across **all possible thresholds**.  \n",
    "\n",
    "- **x-axis:** False Positive Rate (FPR)  (also called 1−Specificity)\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN} = \\frac{FP}{N}\n",
    "$$  \n",
    "\n",
    "- **y-axis:** True Positive Rate (TPR) (also named sensitivity)  \n",
    "$$\n",
    "\\text{TPR} = \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$  \n",
    "\n",
    "- Each point on the curve corresponds to a different **decision threshold** used to convert probabilities into class labels.  \n",
    "\n",
    "- The **diagonal line** represents random guessing; points above it indicate better-than-random performance.\n",
    "\n",
    "- The classifier's performance on the ROC curve is often summarized by a single number: the area under the ROC curve (**AUC-ROC**).\n",
    "\n",
    "- Sklearn provide functions to compute [ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) curve and [AUC-ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). But, we can also do by ourselves.\n",
    "\n",
    "**TASK:** Complete the code below to plot the ROC curve. Specifically, fill out the loop computing tpr and fpr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort scores and corresponding true labels descending\n",
    "desc_order = np.argsort(-probs_hounds)\n",
    "y_true_sorted = gts_hounds[desc_order]\n",
    "y_scores_sorted = probs_hounds[desc_order]\n",
    "\n",
    "# Initialize counts\n",
    "TP, FP = 0, 0,  \n",
    "P, N = np.sum(gts_hounds == 1), np.sum(gts_hounds == 0)\n",
    "\n",
    "\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "for label in y_true_sorted:\n",
    "    # compute current counts\n",
    "    \n",
    "    tpr_list.append(...)\n",
    "    fpr_list.append(...)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "tpr = np.array(tpr_list)\n",
    "fpr = np.array(fpr_list)\n",
    "auc = np.trapezoid(tpr, fpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59cad6",
   "metadata": {},
   "source": [
    "# 3) Classifier Calibration\n",
    "\n",
    "Calibration measures how well a classifier's **predicted probabilities** match the **true likelihood of an event**.  \n",
    "\n",
    "- A perfectly calibrated model outputs probabilities that reflect the actual frequency of positives.  \n",
    "- Example (binary classification):\n",
    "  - If the model predicts **0.7** for 100 samples, roughly **70 of them should belong to the positive class**.\n",
    "  - If it predicts **0.2**, about **20 should be positive**.\n",
    "\n",
    "WARN: High accuracy does not guarantee well-calibrated probabilities, which can make them unreliable for decision-making, risk assessment, or thresholding.\n",
    "\n",
    "\n",
    "Let's plot a calibration curve to check our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bfaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bins\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "bin_indices = np.digitize(probs_hounds, bins) - 1  # get bin index for each probability\n",
    "bin_true_frac = []\n",
    "bin_mean_prob = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = bin_indices == i\n",
    "    mask_sum = np.sum(mask)\n",
    "    bin_counts.append(mask_sum)\n",
    "    if mask_sum > 0:\n",
    "        bin_true_frac.append(np.mean(gts_hounds[mask]))  # fraction of positives\n",
    "        bin_mean_prob.append(np.mean(probs_hounds[mask]))  # mean predicted probability\n",
    "    else:\n",
    "        bin_true_frac.append(np.nan)\n",
    "        bin_mean_prob.append(np.nan)\n",
    "\n",
    "# Convert to arrays\n",
    "bin_true_frac = np.array(bin_true_frac)\n",
    "bin_mean_prob = np.array(bin_mean_prob)\n",
    "bin_counts = np.array(bin_counts)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,8), sharex=True, gridspec_kw={'height_ratios':[3,1]})\n",
    "\n",
    "# Top: calibration curve\n",
    "ax1.plot(bin_mean_prob, bin_true_frac, marker='o', label='Classifier')\n",
    "ax1.plot([0,1], [0,1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "ax1.set_ylabel('Fraction of positives')\n",
    "ax1.set_title('Calibration Curve with Sample Counts')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Bottom: histogram of counts\n",
    "ax2.bar(bin_mean_prob, bin_counts, width=0.08, color='gray')\n",
    "ax2.set_xlabel('Mean predicted probability')\n",
    "ax2.set_ylabel('Number of samples')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e3867",
   "metadata": {},
   "source": [
    "### Overconfident and Underconfident Predictions\n",
    "\n",
    "- **Overconfident:** Predicted probabilities are **too high** compared to actual outcomes.  \n",
    "  - Example: The model predicts 0.9, but only 60% of samples are truly positive.  \n",
    "  - Calibration Curve **below** the diagonal\n",
    "- **Underconfident:** Predicted probabilities are **too low** compared to actual outcomes.  \n",
    "  - Example: The model predicts 0.4, but 70% of samples are truly positive.  \n",
    "  - Calbration Curve **above** the diagonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97a6cb",
   "metadata": {},
   "source": [
    "# Extending Binary Metrics to Multi-class Classification\n",
    "\n",
    "So far, we have discussed metrics, ROC curves, and calibration in the context of **binary classification**.  \n",
    "\n",
    "**Question:**  \n",
    "How could you adapt these concepts to a **multi-class classification problem**? Consider:\n",
    "\n",
    "- How would you compute precision, recall, and F1-score when there are more than two classes?  \n",
    "- How can ROC curves and AUC be extended beyond binary problems?  \n",
    "- How could you assess and visualize calibration for multiple classes?\n",
    "\n",
    "Try to think about strategies like **one-vs-rest**, **macro vs. micro averaging**, and grouping classes.  \n",
    "\n",
    "**Reading:**  \n",
    "- [scikit-learn: Multiclass classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)  \n",
    "- [scikit-learn: ROC curves for multiclass](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)  \n",
    "- [scikit-learn: Calibration of classifiers](https://scikit-learn.org/stable/modules/calibration.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(gts, np.argmax(probs,axis=1), display_labels=test_dataset.classes, xticks_rotation='vertical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifn680",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
